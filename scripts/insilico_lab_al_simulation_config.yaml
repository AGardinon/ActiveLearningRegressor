# Experiment configuration for in-silico lab active learning simulation
experiment_name: "3rep_5c_3pt_mi_ei-tei_ei_protocol"
experiment_notes: "acqui_func.landscape_acquisition() with penalize landscape and predictiong only on candidates."
experiment_evidence:

search_space_variables:
  - "x1"
  - "x2"

target_variables:
  - "Number_of_Particles"

# Cycles parameters
init_batch_size: 3
n_cycles: 5

init_sampling: "fps"
cycle_sampling: "voronoi"

# ML model configuration (GPR)
# Kernel configuration for GPR
# either "RBF_W" or "MATERN_W" or custom recipe
ml_model: "GPR"
kernel_recipe: "RBF_W"

# Additional paramters
model_parameters:
  n_restarts_optimizer: 300
  # alpha: 1e-10
  normalize_y: true
  optimizer: 'fmin_l_bfgs_b'
  use_gridsearch: false
  log_transform: false

data_scaler: "StandardScaler"

# List of acquisition parameters functions to be used in the simulation
acquisition_parameters:
  - acquisition_mode: "expected_improvement"
    percentile: 90

  - acquisition_mode: "target_expected_improvement_800"
    percentile: 90
    y_target: 800
    epsilon: 150

  - acquisition_mode: "target_expected_improvement_1200"
    percentile: 90
    y_target: 1200
    epsilon: 150

  - acquisition_mode: "exploration_mutual_info"
    percentile: 90

landscape_penalization:
  radius: 0.25
  strength: 1.0

# Define stages with specific acquisition modes and cycle lengths 
acquisition_protocol:
  stage_1:
    cycles: 3
    acquisition_modes: ['exploration_mutual_info', 'expected_improvement']
    n_points: [2, 1]  # Number of points to acquire for each mode in stage 1
  stage_2:
    cycles: 2
    acquisition_modes: ['target_expected_improvement_800', 'expected_improvement']
    n_points: [1, 2]

# Path to the ground truth file in the datasets directory
ground_truth_file: "gt_number_of_particles_maternw.csv"
